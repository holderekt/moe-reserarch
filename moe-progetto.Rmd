---
title: |
  | ![](images/uniba-logo.png){width=3in}
  |
  | \textbf{MOE: Mixture Of Experts}
  | Analisi della letteratura e delle Hierarchical MOE
  |
  | \Large Modellazione Statistica - Data Science
  | 

author: "Ivan Diliso - 761053"
header-includes:
- \usepackage[italian]{babel}
- \setcounter{secnumdepth}{3}
- \setcounter{tocdepth}{3}
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{enumitem}
- \usepackage{amsfonts}
- \usepackage{amsmath}
- \usepackage{hyperref}
- \hypersetup{ colorlinks, citecolor=black, filecolor=black, linkcolor=black, urlcolor=black
  }
output:
  
  pdf_document: default
  html_document:
  html_notebook: default
knit: (function(inputFile, encoding) {
      out_dir <- "docs";
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_dir=file.path(dirname(inputFile), out_dir))})
editor_options: 
  markdown: 
    wrap: 72
---


\newpage
\tableofcontents
\newpage

\section{Mixture of Experts}

\subsection{Introduzione}


Le Mixture of Experts (MOE) è una tecnica di enseble learning di
tipologia multi expert (diversi learner che lavorano in parallelo) con
approccio locale (learner selection) applicabile in constesti di
apprendimento supervisionato, come classificazione e regressione, e non
supervisionato come clustering. Nascono nel campo delle reti neurali
nell'ambito dei comining classifiers e ensemble of weak learners. I
modelli MOE utilizzano la metodologia dividi et impera per addestrare
una serie di modelli parametrici e unire i loro ouput per fornire il
risultato finale, questa tipologia di modelli permette di dividere un
task complesso in sottotask, addestrando i vari modelli dell'esempble su
un sottotask diverso. Nella terminologia delle MOE ogni modello
addestrato è chiamato \textbf{esperto}. La caratteristica fondamentale
che distingue le MOE è l'utilizzo di un modello ad hoc chiamato
\textbf{gating} che permette di unire le soluzioni degli esperti ed
assegnare ad ognuno di essi il peso che la loro soluzione avrà nel
comporre la soluzione finale.


\begin{figure}
\centering
\includegraphics[width=8cm]{./images/moe.png}
\caption{MOE con 3 esperti}
\end{figure}

A differenze di altre metodologie ensemble che producono esperti unbiased con
stime di errore non correlate, MOE produce esperti biased con stime correlate
negativamente. Con questa tipologia di modelli è necessaria una conoscenza pregressa
di una divisinoe dei dati, è necessario quindi che il dataset sia divisibile.

\subsection{Experts}

Gli esperti sono i vari modelli addestrati nell'esemble MOE, ogni
esperto andrà a specializzasi nel risolvere uno specifico sottoproblema,
questo si concretizza nello specializzarsi su specifiche porzioni del
feature space. I sottotask creati infatti possono essere sovrapponibili
(più esperti condividono alcune feature) non sovrapponibili (ogni
esperto viene addestrato su un feature space diverso). Ogni esperto
quindi contribuisce in modo diverso nella creazione dell'output finale.
Nelle MOE infatti non è necessario rendere i vari learner dell'esemble
diverse in quanto naturalmente questi lavoreranno su task diversi. Il
problema da risolvere è infatti trovare una divisione naturale dei dati,
questo problema è risolto dall'utilizzo del modello di gating. Per il
partizionamento del feature space, possono essere utilizzate due
metodologie:


\begin{itemize}
  \item Partizionamento \textbf{inplicito}: Feature space viene diviso implicitamente in sottospazi tramite una funzione di errore. L'utilizzo dell'apprendimento tramite backpropagation e il partizionamento dei dati fornito dalla funzione di gating permeette agli esperti di specializzarsi in diversi sottospazi del feature space. In questo caso di parla di  approccio \textbf{MILE} (Mixture of the Implicitly Localised Experts)

  \item Partizionamento \textbf{esplicito}: Si utilizza un algoritmo di clustering per il partizionamento dei dati, questi vengono poi assegnati ad un esperto. In questo caso si parla di un approccio \textbf{MELE} (Mixture of Expliticly Localised Experts)

\end{itemize}

Un esperto può essere qualsiasi modello di apprendimento, nella
formulazione originale gli esperti prendono la forma di modelli di
logistic regression, ma possono essere anche implementati come
percettroni, multi layer perceptron, gaussian mixture, reti neurali
profonde (in questo caso di parla di Deep Mixture of Experts) etc. etc.

\subsection{Gating}

Il modello di gating può essere di diverse tipologie e assolve una
funzione specifica in base alla sua tipologia. Nella formulazione
originale il modello di gating è un modello che, dato in input un
elemento del dataset, fornisce in output una distribuzione di
probabilità sull'insieme di esperti, questa distribuzione può essere
utilizzata in due modi:


\begin{itemize}
\item \textbf{Pooling}: L'esperto a cui è associata la probabilità più alta verrà selezionato come il modello per la predizione dei dati.
\item \textbf{Combining}: Le soluzioni di tutti gli esperti vengono combinate pesate con la probabilità fornita dalla funzione di gating, ogni esperto quindi contribuisce proporzionalmente al peso generato dal gating.
\end{itemize}

Questo approccio associa quindi un diverso peso ad ogni esperto, questa
tecnica può essere vista come una forma di voting dei modelli ensemble,
dove però la capacità di voto può cambiare al variare dell'input. Dato
che la distribuzione è appresa dinamicamente dal modello di gating sulla
base dell'input questa apprende che input assegnare ad ogni esperto
effettuando quindi un partizionamento dei dati. Questo partizionamento è
definito \textbf{hard} se si utilizza una tecnica di pooling,
\textbf{soft} se si utilizza il combining.



\subsection{Hierarchical Mixture of Experts}

Le Hierrarchical Mixture of Experts (HMOE) sono una estensione dei modelli MOE in cui rimpiazzo ogni esperto con un sistema completo MOE in modo ricorsivo creando una struttura ad albero in cui i nodi interni saranno le varie funzioni di gating sviluppate in modo gerarchico, e i nodi foglia conterranno gli esperti. Può essere interpretato come un albero di decisione in cui i nodi interni (gating) fungono da nodi di decisione e la decisione finale è un singolo nodo foglia (in caso di pooling) o una combinazione pesata dei nodi foglia (in caso di combining). Se nei modelli MOE un parametro importante per l'ottimizzazione del modello è il numero di esperti da utilizzare in questo caso il parametro è la profondità della ricorsione, che determinerà il numero di esperti utilizzati. 



\begin{figure}
\centering
\includegraphics[width=9.5cm]{./images/hmoe.png}
\caption{HMOE con con profondità 2}
\end{figure}



Questa tipologia di albero, con gating di tipo combining viene definita \textbf{soft decision tree} in quanto
a differenze degli alberi di decisione hard, vengono utilizzate le distribuzioni di probabilità generate dai nodi gating per esplorare tutti i possibili percorsi dell'albero, per poi effettuare una somma pesata a livello di foglie, dove ogni peso è uguale al prodotto dei valor idi gating di ogni path per arrivare alla foglia. A differenza dei modelli CART (Classification And Regression Trees) dove ogni nodo contiene un valore costante per la decisione, nei modelli HMO ogni nodo implementa un modello lineare (o di regressione logistica). L'uso di questa formulazione soft permette di catturare situazioni in cui la transizione da una risposta alta ad una bassa è graduale.




\section{Apprendimento e backpropagation}

\subsection{Modello MOE}

Sia $x \in \mathbb{R}^n$ vettore di input, sia $T$ il numero di esperti
modello, $h_1, \dots, h_T$ gli esperti del modello e $y$ variabile
target. Dati $W_i$ parametri dell'i-esimo esperto, l'obiettivo del modello è
approssimare la distribuzione di $y$ sulla base dei dati di training 

$$
h_i(y|x; W_i)
$$

La funzione di gating produce un set di coefficenti the pesano il
contributo degli esperti, sia $v_i$ vettore dei pesi della funzione di
gating relativa all'i-esimo esperto e sia $\alpha$ insieme dei paramatri dell modello di
gating (insieme dei pesi relativi ad ogni espeto). Essendo una distribuzione di probabilità la somma dei coefficenti su tutti gli esperti dovrà essere uguale a 1. Definiamo quindi il set di coefficenti generati dalla funzione di gating, sull'i-esimo esperto, dato l'insieme di parametri $\alpha$ come:
$$
\pi_i(x; \alpha) : \sum_{i=1}^{T} \pi_i(x; \alpha) = 1
$$ 

Sulla base di queste probabilità partizioniamo lo spazio di input,
diverse partizioni appartengono a diversi esperti. L'output del modello, dato $\psi$ insieme dei parametri degli esperti e del modello di gating, sarà quindi:

$$
H(y |x; \psi) = \sum_{i=1}^{T} \pi_i(x; \alpha) \cdot h_i(y|x; W_i)
$$

Nella fase di training il valore $\pi_i(x; \alpha)$ indica la
probabilità che l'istanza $x$ appaia nel traning set dell'i-esimo
esperto. Mentre nella fase di testing definisce il contributo che il modello $h_i$
fornisce alla predizione finale. L'output della funzione di gating può essere
espresso tramite una softmax:

$$
\pi_i(x; \alpha) = \frac{e^{v_i}x}{\sum_{l=1}^{k}e^{v_l}x} 
$$ 

usata sia per classificazione che per regressione.

\subsection{Modello HMOE}

Seguendo il formalismo definito in precedenza definiamo un modello HMOE di profondità 2. Definito $I$ numero di nodi connessi al nodo gating al livello di
radice, $J_i$ numero di nodi connessi all'i-esimo nodo gating, $\pi_i$
ougput del nodo gating al livello di radice e $\pi_{j|i}$ output dell j-esimo
nodo gating connesso all'i-esimo nodo gating, l'ouput del modello HMOE sarà:

$$
H(y |x; \psi) = \sum_{i=1}^{I} \pi_i(x; \alpha_{\pi_i}) \cdot \sum_{j=1}^{J_i} \pi_{j|i}(x; \alpha_{\pi_{j|i}}) h_{ij}(y|x; W_{ij})
$$

Nella fase di apprendimento e testing i  dati sono dati in input agli esperti che producono dei vettori di output che procedono nell'albero verso alto (in direzione opposta rispetto ai modelli CART), vengono moltiplicati tra loro e sommati seguendo i vari livelli dell'albero.


\subsection{Apprendimento}

Gating network alloca dati di training a uno o più esperti e se l'output
è incorretto il cambiamento dei pesi e localizzato su questo esperto.
Locale in quasnto i pesi di un esperto sono disaccoppiati dai pesi di un
altr oesperto.

Può avvenire tramite:


\begin{itemize}
  \item GRADIENT DESCENT: Particolarmente utile con i mixuture of multi layer perceptron experts. Addestramento utilizzando questa funzione tende ad assegnare un dato di training ad ogni esperto
  \item EXPECTATION MAXIMIZATION: Metodi ME cercano di risolvere due task, dato un esperto trovare la funzione di gating ottimale e data la funzinoe di gating addestrare ogni esperto a massimizzare le performance sulla distribuzione assegnata dalla funzione di gating, questo rende naturale l'utilizzo di un algoritmo di expectation maximization 
\end{itemize}

\subsection{Funzioni di errore}

$$
E = \lVert y - \sum_{j}g_j O_j \rVert^2
$$

I pesi di ogni esperto sono cosi aggiornati sulla base di un errore
ensemble totale che sulla base dell'errore dello specifico esperto.
Questo permette un alto livello di cooperazione e tende a sfruttare
quasi tutti gli esperti del modello (nessun esperto non contribuisce al
problema) In questa funzione di errore si assume che che l'output del
sistema sia una combinazione lineare degli output degli esperti locali,
con il gating che determina la proporzione. Strong coupling dei pesi.

$$
E =   \sum_{j}g_j \lVert y -O_j \rVert^2
$$ Pesi aggiornati su errori singoli, non assicura la localizzazione
degli esperti.

\subsection{Errore con gaussian mixture}

Una misura di errore che tiene conto di entrambi i fattori è basata
sulla negative log probability di generare l'output vector desiderato,
se si assume una mixture di modelli gaussiani con $\sum$ matrice di
covarianza

$$
E_{ME} = -log\sum_{j}g_j e^{-\frac{1}{2} (y- O_j)^T \sum^{-1}(y-O_j)}
$$ L'apprendimento di ogni esperto avviene sull'errore individuale, ma
l'aggiornamento dei pesi per ogni esperto è propozionale all suo rateo
di errore sull'errore totale. Questo permette la localizzazione degli
esperti nei sotto spazio delel feature corrispondente

\subsection{Errore con MLP-Experts}

Ogni esperto è un MLP con un hidden layer che produce un output $O_j$ in
funzione dell'inpnt con funzione di attivazione sigmoidale.
Apprendimento con backpropagation massimizzando la log likelihood dei
dati i parametri.



\subsection{Model selection}

Ottimizzazione di iperparamtrei, depth e connessioni dell'albero. Simile
alla model selection applicata ad alberi, modificata la funzione di
valutazione di un branch dell'albero.


\begin{itemize}
\item Modelli growing: aggiungo layer all'albero e determno  la profondità e numero di esperti
\item Pruning modelli: Riduzione dei requirement computazionali. Parametri costanti ma considero le path più probabili. Pruno i banch meno usati
\end{itemize}




\section{MOE e HMOE in R}

MEclusternet, flexmix, mixreg, mixtools, flexCWM, meteorist

\subsection{mixtools: Tools for Analyzing Finite Mixture Models}

\subsection{MoEClust}

La libreria MoEClust(Gaussian Parsimonious Clustering Models with Gating
and Expert Network Covariates and a Noise Component) permette di
addestrare mixture of expert gaussiane finite tramite algoritmo di
ottimizzazione expectation maximization, l'utilizzo principale della
libreria è il clustering.

\section{Clutering asteroidi pericolosi che orbitano la terra}

\subsection{Dataset}


\begin{itemize}
\item Neo Reference ID e Name
\item Absolute magnitude
\item Est Dia: Stima del diametro dell'asteroide, vengono fornite le misure di massimo e minimo in KM, M, Miles  e Feet. Feature ridondanti mantengo solo la stima massima e minima dell'asteroide in KM.
\item Approach date: In format di data e epoch, vengono rimosse in quanto non utili al clustering
\item Velocità relativa: In km/h, in km/s e in miles/h: Viene mantenuta solo la velocità in km/s
\item Distanza mancante: In unità astronomica, kilometri, lunare e miglia. Viene mantanuta la distanza astronomicaz
\item Pianeta orbitante: Contiene il valore terra per tutti i dati, id dell'orbita, giorno di verifica dell'orbita e incertezza dell'orbita e intersezione minima con le orbite
\item Jupiter tisserand invariant: Descrive la dinamica orbitale dell'asteroide in ralazione a Jupyter, fornisce una misura del grado di stabilità dell'orbita, valori elevati indicano orbite stabili, basse instabili e soggette a perturbazioni.
\item epoch osculation 
\item eccentricity
\item semi major axis
\item inclination
\item asc node longitude
\item orbital period
\item perihelion distance
\item periohelion arg
\item aphelion dist
\item perihelion time
\item mean nomaly
\item mean motion
\item equinox
\item hazadous
\end{itemize}

\subsection{Rimozione feature}

```{r}
eval = FALSE
library(readr)
library(ggplot2)
library(gpairs)
library(GGally)

nasa <- read_csv("data/nasa.csv")

dataset <- nasa[1:100, - (c((1:2), (6:11), (12:13), (15:16), (18:25), 27, 31,34, (36:39))), drop=FALSE]
dataset$Hazardous <- as.numeric(as.logical(dataset$Hazardous))


```

```{r}
eval=FALSE
cols <- character(nrow(dataset))
cols[dataset$Hazardous == 0] <- "cyan4"
cols[dataset$Hazardous == 1] <- "darkorange1"

plot(dataset[, c(1:6, 13)], col=cols)
plot(dataset[, c(7:12, 13)], col=cols)
```

```{r}
eval = FALSE
g1 <- ggpairs(data=dataset[, c(1:6, 13)], mapping=ggplot2::aes(colour = as.logical(Hazardous)),lower=list(combo=wrap("facethist",binwidth=1)))

g1
```

```{r}
eval=FALSE
library(MoEClust)
gating <- dataset$Hazardous
mod1 <- MoE_stepwise(dataset, gating)

mod1
```


```{r}
(summ <- summary(mod1, classification=TRUE, parameters=FALSE, networks=TRUE))
```

```{r}
eval = FALSE
plot(mod1, what="gpairs", jitter=FALSE)
```

```{r}
eval= FALSE
library(MoEClust)
library(caret)
train_ind <- sample(1:nrow(dataset), 70)
train_set <- dataset[train_ind, ]
test_set <- dataset[-train_ind,, drop=FALSE]
haz = train_set$Hazardous

model <- MoE_clust(data = train_set, G=5, equalPro = FALSE, gating = ~ Hazardous, network.data = train_set)

a <- predict(model, test_set[,], use.y = FALSE)[1:9]$MAPy[,13]

a <- as.factor(as.numeric((a > 0.4)))
b <- as.factor(test_set$Hazardous)



example <- confusionMatrix(data=a, reference = b)

example
```

```{=tex}
\newpage
\begin{thebibliography}{9}

    \bibitem{esl} 
    Hastie, T., Tibshirani, R., \& Friedman, J. (2009). 
    The elements of statistical learning: data mining, 
    inference, and prediction. Springer Science \& Business Media.
    
    \bibitem{iml} 
    Alpaydin, E. (2020). Introduction to machine learning. MIT press.
    
    \bibitem{efa}
    Zhou, Z. H. (2012). Ensemble methods: foundations and algorithms. CRC press.
    
    \bibitem{ema}
    Zhang, C., \& Ma, Y. (Eds.). (2012). Ensemble machine learning: methods and applications. Springer Science \&         Business Media.
    
    \bibitem{pri}
    Bishop, C. M., \& Nasrabadi, N. M. (2006). Pattern recognition and machine learning (Vol. 4, No. 4, p. 738). New York: springer.
    
    \bibitem{litera}
    Masoudnia, S., \& Ebrahimpour, R. (2014). Mixture of experts: a literature survey. The Artificial Intelligence Review, 42(2), 275.
    
    \bibitem{amiim}
    Jacobs, R. A., Jordan, M. I., Nowlan, S. J., \& Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.
    
    
    \bibitem{twenty}
    Yuksel, S. E., Wilson, J. N., \& Gader, P. D. (2012). Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems, 23(8), 1177-1193.
    
    
    \bibitem{hme}
    
    Jordan, M. I., \& Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. Neural computation, 6(2), 181-214.
    
    \bibitem{models}
    
    Gormley, I. C.,   \& Frühwirth-Schnatter, S. (2019). Mixture of experts models. In Handbook of mixture analysis (pp. 271-307). Chapman and Hall/CRC.

    
\end{thebibliography}
```
